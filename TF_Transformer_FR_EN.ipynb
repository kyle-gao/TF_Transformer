{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF_Transformer_FR_EN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPUFuoZGcZZNdl8ynR4fOOj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyle-gao/TF_Transformer/blob/master/TF_Transformer_FR_EN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anQuvdlJJ8_r",
        "colab_type": "text"
      },
      "source": [
        "Copyright 2020 Yi Lin(Kyle) Gao\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_qNSzzyaCbD",
        "colab_type": "text"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "jmjh290raIky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFKnTnn7Odcp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import time"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksjz9m2nXDaz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dense_dim = 256\n",
        "num_heads = 8\n",
        "\n",
        "\n",
        "max_len_en = 50\n",
        "min_len_en = 10  # The transcript has many short lines indicating the date or speaker, which we should filter out.\n",
        "max_len_fr = 50\n",
        "min_len_fr = 10\n",
        "\n",
        "eng_path = \"Data/en.txt\"\n",
        "fr_path = \"Data/fr.txt\"\n",
        "EPOCHS = 20\n",
        "en_ds = tf.data.TextLineDataset(eng_path)\n",
        "fr_ds = tf.data.TextLineDataset(fr_path)\n",
        "ds = tf.data.Dataset.zip((fr_ds, en_ds))\n",
        "\n",
        "tokenizer_en = tfds.features.text.SubwordTextEncoder.load_from_file(\"Data/tokenizer_en\")\n",
        "tokenizer_fr = tfds.features.text.SubwordTextEncoder.load_from_file(\"Data/tokenizer_fr\")\n",
        "en_vocab_size = tokenizer_en.vocab_size + 2\n",
        "fr_vocab_size = tokenizer_fr.vocab_size + 2"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFvRld90XAKc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode(fr, eng):\n",
        "    # Adds start token (tokenizer.vocab_size) and end token (tokenizer.vocab_size + 1) to (question,answer)\n",
        "    question = [tokenizer_fr.vocab_size] + tokenizer_fr.encode(fr.numpy()) + [tokenizer_fr.vocab_size + 1]\n",
        "    answer = [tokenizer_en.vocab_size] + tokenizer_en.encode(eng.numpy()) + [tokenizer_en.vocab_size + 1]\n",
        "\n",
        "    return question, answer\n",
        "\n",
        "\n",
        "def tf_interleave_encode(question, answer):\n",
        "    # We have to wrap encode in a tf.py_function() and return a Dataset so we can use Dataset.interleave()\n",
        "    question, answer = tf.py_function(encode, [question, answer], [tf.int64, tf.int64])\n",
        "    question.set_shape([None])\n",
        "    answer.set_shape([None])\n",
        "\n",
        "    return tf.data.Dataset.from_tensors((question, answer))\n",
        "\n",
        "\n",
        "def filter_max_length(x, y, max_length_question=max_len_fr, max_length_answer=max_len_en):\n",
        "    return tf.logical_and(tf.size(x) <= max_length_question,\n",
        "                          tf.size(y) <= max_length_answer)\n",
        "\n",
        "\n",
        "def filter_min_length(x, y, min_len_question=min_len_fr, min_len_answer=min_len_en):\n",
        "    return tf.logical_and(tf.size(x) >= min_len_question,\n",
        "                          tf.size(y) >= min_len_answer)\n",
        "\n",
        "\n",
        "def preprocess(dataset, batch_size, pad_len_question=max_len_fr, pad_length_answer=max_len_en):\n",
        "    dataset = dataset.cache()\n",
        "    # dataset = dataset.map(tf_encode)\n",
        "    dataset = dataset.interleave(tf_interleave_encode, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    dataset = dataset.filter(filter_max_length)\n",
        "    dataset = dataset.filter(filter_min_length)\n",
        "    dataset = dataset.shuffle(10000)\n",
        "\n",
        "    dataset = dataset.padded_batch(batch_size, drop_remainder=True,\n",
        "                                   padded_shapes=([pad_len_question], [pad_length_answer]))\n",
        "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return dataset"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXcg2N4yYKur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = preprocess(ds, 64)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_1FxrOUYEo9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def positional_encoding(pos, d_model):\n",
        "    \"\"\"\n",
        "    :param pos: int max position\n",
        "    :param d_model: dimension of the model\n",
        "    :return: (1,pos,d_model) array of sinusoidal positional encoding\n",
        "    \"\"\"\n",
        "    pos_enc = np.zeros((1, pos, d_model))\n",
        "    for p in range(pos):\n",
        "        for i in range(d_model // 2):\n",
        "            angles = p / np.power(10000, (2 * i) / np.float32(d_model))\n",
        "            pos_enc[:, p, 2 * i] = np.sin(angles)\n",
        "            pos_enc[:, p, 2 * i + 1] = np.cos(angles)\n",
        "        if d_model % 2 == 1:\n",
        "            # if d_model is odd loop doesn't hit last even index\n",
        "            angles = p / np.power(10000, (2 * d_model) / np.float32(d_model))\n",
        "            pos_enc[:, p, d_model - 1] = np.sin(angles)\n",
        "    return tf.cast(pos_enc, tf.float32)\n",
        "\n",
        "\n",
        "def padding_mask(seq):\n",
        "    # Returns (batch, seq_len, 1, 1) tensor with 0's where the sequence is padded, 1 where it is not\n",
        "\n",
        "    mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return mask[:, tf.newaxis, :,  tf.newaxis]  # (batch, 1, seq_len, 1) m l j h  <- j gets masked\n",
        "\n",
        "\n",
        "def forward_mask(seq):\n",
        "    \"\"\"\n",
        "    Calculates a combined forward mask and padding mask for a batch of sequences\n",
        "    :param seq: (batch,seq_len) a batch of sequences\n",
        "    :return:  a combined look_ahead_mask (lower triangular 1s)\n",
        "    and padding mask (batch, seq_len, seq_len, 1)\n",
        "    \"\"\"\n",
        "    seq_len = tf.shape(seq)[1]\n",
        "\n",
        "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "    look_ahead_mask = look_ahead_mask[tf.newaxis, :, :, tf.newaxis]  # (batch, seq_len, seq_len, 1)\n",
        "\n",
        "    padded_mask = padding_mask(seq)\n",
        "\n",
        "    # return padded_mask * look_ahead_mask  # (batch, seq_len, seq_len, 1)\n",
        "    return tf.maximum(padded_mask, look_ahead_mask)\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwFRZhdwWwK_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "    \"\"\"Implemented with tf.einsum(), is faster than using tf.transpose() with tf.matmul()\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads,depth)\n",
        "\n",
        "        Arguments:\n",
        "        x -- A tokenized sequence (batch_size, seq_len, d_model)\n",
        "\n",
        "        Returns:\n",
        "        A tokenized sequence with dimensions (batch_size, seq_len, num_heads, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "\n",
        "        return x\n",
        "\n",
        "    def call(self, q, k, v, mask=None):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "        q = self.wq(q)  # (batch_size,len_q, dim_q)\n",
        "        k = self.wk(k)  # (batch_size,len_v, dim_q)\n",
        "        v = self.wv(v)  # (batch_size,len_v, dim_v)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, len_q, num_heads, depth_q) (m,l,h,d)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, len_v, num_heads, depth_q) (m,j,h,d)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, len_v, num_heads, depth_v) (m,j,h,e)\n",
        "\n",
        "        qk = tf.einsum(\"mlhd,mjhd->mljh\", q, k)  # (batch_size, len_q, len_v, num_heads) (m,l,j,h)\n",
        "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "        qk = qk / tf.math.sqrt(dk)\n",
        "\n",
        "        if mask is not None:\n",
        "            qk = qk - mask*1e9 # We are using a multiplicative mask\n",
        "\n",
        "        qk = tf.nn.softmax(qk, axis=-2)  # (batch_size,len_q,len_v, num_heads) (m,l,j,h)\n",
        "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "        qk = qk / tf.math.sqrt(dk)\n",
        "\n",
        "        output = tf.einsum(\"mljh, mjhe -> mlhe\", qk, v)  # (batch_size,len_q, heads, depth_v)\n",
        "        output = tf.reshape(output, (batch_size, -1, self.num_heads * self.depth))  # (batch_size,len_q, d_model)\n",
        "\n",
        "        return self.dense(output)\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"The EncoderLayer consists of one MultiHeadAttention layer connected to a FeedForward layer,\n",
        "    each of these 2 layers have a residual connection.\"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, d_model, dense_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.dense = tf.keras.Sequential([tf.keras.layers.Dense(dense_dim, activation='relu'),\n",
        "                                          tf.keras.layers.Dense(d_model)])\n",
        "\n",
        "        self.norm1 = tf.keras.layers.LayerNormalization()\n",
        "        self.norm2 = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(dropout)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        out_attention = self.attention(x, x, x, mask)  # (batch_size,seq_len,d_model)\n",
        "        out_attention = self.dropout1(out_attention, training=training)\n",
        "        out1 = self.norm1(x + out_attention)  # residual connection (batch_size,seq_len,d_model)\n",
        "\n",
        "        out_dense = self.dense(out1)  # (batch_size,seq_len,d_model)\n",
        "        out2 = self.norm2(out1 + out_dense)  # residual conenction (batch_size,seq_len,d_model)\n",
        "        return out2\n",
        "\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    \"\"\"The Encoder consists of EncoderLayer\"\"\"\n",
        "\n",
        "    def __init__(self, num_layers, num_heads, d_model, dense_dim,\n",
        "                 vocab_size, max_encoding_position, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = positional_encoding(max_encoding_position, d_model)\n",
        "        self.encoding_layers = [EncoderLayer(num_heads, d_model, dense_dim, dropout) for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, x, training, mask=None):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        x = self.embedding(x)  # (batch_size,input_len,d_model)\n",
        "        x = x * tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x = x + self.positional_encoding[:, :seq_len, :]\n",
        "        x = self.dropout(x, training=training)\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.encoding_layers[i](x, training, mask)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        return x\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"A decoder layers consists of two MultiHeadAttention, one for the Decoder input, one from Encoder output\"\"\"\n",
        "    def __init__(self, num_heads, d_model, dense_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attention1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.attention2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.dense = tf.keras.Sequential([tf.keras.layers.Dense(dense_dim, activation='relu'),\n",
        "                                          tf.keras.layers.Dense(d_model)])\n",
        "\n",
        "        self.norm1 = tf.keras.layers.LayerNormalization()\n",
        "        self.norm2 = tf.keras.layers.LayerNormalization()\n",
        "        self.norm3 = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(dropout)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(dropout)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, encoder_out, x, training, forward_mask, padding_mask):\n",
        "\n",
        "        out_attention1 = self.attention1(x, x, x,\n",
        "                                         forward_mask)  # (batch_size, seq_len_answer, d_model) -> The return seq_len is the same as that of the first argument of the call.\n",
        "        out_attention1 = self.dropout1(out_attention1, training=training)\n",
        "        out1 = self.norm1(x + out_attention1)  # residual connection (batch_size, seq_len_answer, d_model)\n",
        "\n",
        "        out_attention2 = self.attention2(out1, encoder_out, encoder_out,\n",
        "                                         padding_mask)  # (batch_size, seq_len_answer, d_model)\n",
        "        out_attention2 = self.dropout2(out_attention2, training=training)\n",
        "        out2 = self.norm2(out1 + out_attention2)\n",
        "\n",
        "        out_dense = self.dense(out2)\n",
        "        out_dense = self.dropout3(out_dense + out2)\n",
        "\n",
        "        return out_dense\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    \"\"\"The Decoder consists of multiple DecoderLayer\"\"\"\n",
        "    def __init__(self, num_layers, num_heads, d_model, dense_dim,\n",
        "                 vocab_size, max_encoding_position, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = positional_encoding(max_encoding_position, d_model)\n",
        "        self.decoder_layers = [DecoderLayer(num_heads, d_model, dense_dim, dropout) for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, encoder_out, x, training, forward_mask=None, padding_mask=None):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        x = self.embedding(x)  # (batch_size,input_len,d_model)\n",
        "        x = x * tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x = x + self.positional_encoding[:, :seq_len, :]\n",
        "        x = self.dropout(x, training=training)\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.decoder_layers[i](encoder_out, x, training, forward_mask,\n",
        "                                       padding_mask)  # (batch_size, input_seq_len, d_model)\n",
        "        return x\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, num_layers, num_heads, d_model, dense_dim, in_vocab_size, tar_vocab_size,\n",
        "                 input_max_position, target_max_position, rate=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, num_heads, d_model, dense_dim,\n",
        "                               in_vocab_size, max_encoding_position=input_max_position, dropout=0.1)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, num_heads, d_model, dense_dim,\n",
        "                               tar_vocab_size, max_encoding_position=target_max_position, dropout=0.1)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(tar_vocab_size)\n",
        "\n",
        "    def call(self, input, target, training=False, enc_mask=None, dec_forward_mask=None, dec_padding_mask=None):\n",
        "        out_encoder = self.encoder(input, training=training, mask=enc_mask)\n",
        "\n",
        "        out_decoder = self.decoder(out_encoder, target, training=training, forward_mask=dec_forward_mask,\n",
        "                                   padding_mask=dec_padding_mask)\n",
        "\n",
        "        out = self.dense(out_decoder)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTSPidClYyzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer = Transformer(num_layers=num_layers, num_heads=num_heads, d_model=d_model, dense_dim=dense_dim,\n",
        "                          in_vocab_size=fr_vocab_size, tar_vocab_size=en_vocab_size,\n",
        "                          input_max_position=max_len_fr, target_max_position=max_len_en, rate=0.1)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQlMu9Z_WzpI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(question):\n",
        "\n",
        "    start_token = [tokenizer_fr.vocab_size]\n",
        "    end_token = [tokenizer_fr.vocab_size + 1]\n",
        "    question = start_token + tokenizer_fr.encode(question) + end_token\n",
        "    question = tf.expand_dims(question, 0)\n",
        "    answer_in = [tokenizer_en.vocab_size]\n",
        "    answer_in = tf.expand_dims(answer_in, 0)\n",
        "\n",
        "    for i in range(max_len_fr):\n",
        "        enc_padding_mask = padding_mask(question)\n",
        "        dec_padding_mask = padding_mask(question)\n",
        "        dec_forward_mask = forward_mask(answer_in)\n",
        "\n",
        "        predictions = transformer(question, answer_in, training=False, enc_mask=enc_padding_mask,\n",
        "                                  dec_forward_mask=dec_forward_mask, dec_padding_mask=dec_padding_mask)\n",
        "        prediction = predictions[:, -1, :]  # select the last word to add to the outputs\n",
        "\n",
        "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
        "\n",
        "        if predicted_id == tokenizer_en.vocab_size + 1:\n",
        "            return tf.squeeze(answer_in, axis=0)\n",
        "        predicted_id = tf.expand_dims(predicted_id, 0)\n",
        "        answer_in = tf.concat([answer_in, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(answer_in, axis=0)\n",
        "\n",
        "\n",
        "def translate(sentence):\n",
        "    result = np.array(evaluate(sentence))\n",
        "\n",
        "    predicted_sentence = tokenizer_en.decode([i for i in result\n",
        "                                              if tokenizer_en.vocab_size > i > 0])\n",
        "    print('Input: {}'.format(sentence))\n",
        "    print('Predicted answer: {}'.format(predicted_sentence))\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABu70dFYXSUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "        def __init__(self, d_model, warmup_steps=4000):\n",
        "            super(CustomSchedule, self).__init__()\n",
        "\n",
        "            self.d_model = d_model\n",
        "            self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "            self.warmup_steps = warmup_steps\n",
        "\n",
        "        def __call__(self, step):\n",
        "            arg1 = tf.math.rsqrt(step)\n",
        "            arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "            return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "learning_rate = CustomSchedule(d_model, warmup_steps=4000)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
        "                                         epsilon=1e-9)\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "\n",
        "def masked_loss_fn(answer, prediction):\n",
        "        mask = tf.math.logical_not(tf.math.equal(answer, 0))  # 0 at zeroes, 1 at non-zeroes since seq is padded\n",
        "        # mask = tf.math.equal(answer, 0)\n",
        "        mask = tf.cast(mask, tf.int32)\n",
        "        loss_value = loss_fn(answer, prediction,\n",
        "                             sample_weight=mask)  # set the zeros to zero weight, other values have weight of 1.\n",
        "\n",
        "        return loss_value\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "        name='train_accuracy')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OryxpF_Xa11",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "525ff13c-b748-4c7c-9be9-614265758f12"
      },
      "source": [
        "    signature = [tf.TensorSpec(shape=(None, max_len_fr), dtype=tf.int64),\n",
        "                 tf.TensorSpec(shape=(None, max_len_en),\n",
        "                               dtype=tf.int64), ]  # a bit faster if we specify the signature\n",
        "\n",
        "    @tf.function(input_signature=signature)\n",
        "    def train_step(question, answer):\n",
        "        answer_in = answer[:, :-1]\n",
        "        answer_tar = answer[:, 1:]\n",
        "\n",
        "        enc_padding_mask = padding_mask(question)\n",
        "        dec_padding_mask = padding_mask(question)\n",
        "        dec_forward_mask = forward_mask(answer_in)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = transformer(question, answer_in, training=True, enc_mask=enc_padding_mask,\n",
        "                                      dec_forward_mask=dec_forward_mask, dec_padding_mask=dec_padding_mask)\n",
        "            loss = masked_loss_fn(answer_tar, predictions)\n",
        "\n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "        train_loss(loss)\n",
        "        train_accuracy(answer_tar, predictions)\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        start = time.time()\n",
        "\n",
        "        train_loss.reset_states()\n",
        "        train_accuracy.reset_states()\n",
        "\n",
        "        for (batch, (question, answer)) in enumerate(train_dataset):\n",
        "            train_step(question, answer)\n",
        "\n",
        "        print('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1,\n",
        "                                                            train_loss.result(),\n",
        "                                                            train_accuracy.result()))\n",
        "\n",
        "        print('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n",
        "        translate(\"son honneur le president informe le senat que des senateurs attendent a la porte pour etre \"\n",
        "                        \"presentes\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss 1.2962 Accuracy 0.2337\n",
            "Time taken for 1 epoch: 192.4174439907074 secs\n",
            "\n",
            "Input: son honneur le president informe le senat que des senateurs attendent a la porte pour etre presentes\n",
            "Predicted answer: the honour the speaker informed the senate that senators are going to be introduced \n",
            "Epoch 2 Loss 1.2025 Accuracy 0.2457\n",
            "Time taken for 1 epoch: 187.0682246685028 secs\n",
            "\n",
            "Input: son honneur le president informe le senat que des senateurs attendent a la porte pour etre presentes\n",
            "Predicted answer: the speaker informed the senate that senators are waiting for the senate to be introduced \n",
            "Epoch 3 Loss 1.1376 Accuracy 0.2543\n",
            "Time taken for 1 epoch: 186.88165593147278 secs\n",
            "\n",
            "Input: son honneur le president informe le senat que des senateurs attendent a la porte pour etre presentes\n",
            "Predicted answer: his honour informed the senate that senators are waiting for the senate \n",
            "Epoch 4 Loss 1.0909 Accuracy 0.2605\n",
            "Time taken for 1 epoch: 186.60118699073792 secs\n",
            "\n",
            "Input: son honneur le president informe le senat que des senateurs attendent a la porte pour etre presentes\n",
            "Predicted answer: the speaker informed the senate that senators are waiting to be before the speaker to be introduced \n",
            "Epoch 5 Loss 1.0548 Accuracy 0.2654\n",
            "Time taken for 1 epoch: 186.44689869880676 secs\n",
            "\n",
            "Input: son honneur le president informe le senat que des senateurs attendent a la porte pour etre presentes\n",
            "Predicted answer: the hon the speaker informed the senate that senators are waiting to be introduced \n",
            "Epoch 6 Loss 1.0267 Accuracy 0.2693\n",
            "Time taken for 1 epoch: 186.76488876342773 secs\n",
            "\n",
            "Input: son honneur le president informe le senat que des senateurs attendent a la porte pour etre presentes\n",
            "Predicted answer: the hon the speaker informed the senate that senators are waiting to be introduced \n",
            "Epoch 7 Loss 1.0033 Accuracy 0.2725\n",
            "Time taken for 1 epoch: 188.05754899978638 secs\n",
            "\n",
            "Input: son honneur le president informe le senat que des senateurs attendent a la porte pour etre presentes\n",
            "Predicted answer: the hon the speaker informed the senate that senators are waiting to be waiting for the door to be introduced \n",
            "Epoch 8 Loss 0.9833 Accuracy 0.2752\n",
            "Time taken for 1 epoch: 187.47811770439148 secs\n",
            "\n",
            "Input: son honneur le president informe le senat que des senateurs attendent a la porte pour etre presentes\n",
            "Predicted answer: the hon the speaker informed the senate that senators are waiting to be brought forward \n",
            "Epoch 9 Loss 0.9670 Accuracy 0.2775\n",
            "Time taken for 1 epoch: 187.3516354560852 secs\n",
            "\n",
            "Input: son honneur le president informe le senat que des senateurs attendent a la porte pour etre presentes\n",
            "Predicted answer: the hon the speaker informed the senate that senators are waiting to be taken to be brought forward \n",
            "Epoch 10 Loss 0.9522 Accuracy 0.2795\n",
            "Time taken for 1 epoch: 187.8548846244812 secs\n",
            "\n",
            "Input: son honneur le president informe le senat que des senateurs attendent a la porte pour etre presentes\n",
            "Predicted answer: the hon the speaker informed the senate that senators are waiting for the door to be introduced \n",
            "Epoch 11 Loss 0.9391 Accuracy 0.2815\n",
            "Time taken for 1 epoch: 187.25289106369019 secs\n",
            "\n",
            "Input: son honneur le president informe le senat que des senateurs attendent a la porte pour etre presentes\n",
            "Predicted answer: the hon the speaker informed the senate that senators are waiting to be introduced \n",
            "Epoch 12 Loss 0.9276 Accuracy 0.2831\n",
            "Time taken for 1 epoch: 186.8608386516571 secs\n",
            "\n",
            "Input: son honneur le president informe le senat que des senateurs attendent a la porte pour etre presentes\n",
            "Predicted answer: the hon the speaker informed the senate that senators are waiting to wait for the door to being presented \n",
            "Epoch 13 Loss 0.9171 Accuracy 0.2847\n",
            "Time taken for 1 epoch: 187.1262481212616 secs\n",
            "\n",
            "Input: son honneur le president informe le senat que des senateurs attendent a la porte pour etre presentes\n",
            "Predicted answer: the hon the speaker informed the senate that senators are waiting to be brought before us \n",
            "Epoch 14 Loss 0.9083 Accuracy 0.2858\n",
            "Time taken for 1 epoch: 187.47323155403137 secs\n",
            "\n",
            "Input: son honneur le president informe le senat que des senateurs attendent a la porte pour etre presentes\n",
            "Predicted answer: the hon the speaker informed the senate that senators are waiting to be brought forward \n",
            "Epoch 15 Loss 0.8993 Accuracy 0.2872\n",
            "Time taken for 1 epoch: 187.72181272506714 secs\n",
            "\n",
            "Input: son honneur le president informe le senat que des senateurs attendent a la porte pour etre presentes\n",
            "Predicted answer: the hon the speaker informed the senate that senators are waiting to be brought forward \n",
            "Epoch 16 Loss 0.8923 Accuracy 0.2881\n",
            "Time taken for 1 epoch: 187.6087348461151 secs\n",
            "\n",
            "Input: son honneur le president informe le senat que des senateurs attendent a la porte pour etre presentes\n",
            "Predicted answer: the hon the speaker informed the senate that senators shall be waiting for the door to be presented \n",
            "Epoch 17 Loss 0.8856 Accuracy 0.2890\n",
            "Time taken for 1 epoch: 187.59232687950134 secs\n",
            "\n",
            "Input: son honneur le president informe le senat que des senateurs attendent a la porte pour etre presentes\n",
            "Predicted answer: the hon the speaker informed the senate that senators are waiting for the door to be introduced \n",
            "Epoch 18 Loss 0.8785 Accuracy 0.2902\n",
            "Time taken for 1 epoch: 187.73578429222107 secs\n",
            "\n",
            "Input: son honneur le president informe le senat que des senateurs attendent a la porte pour etre presentes\n",
            "Predicted answer: the hon the speaker informed the senate that senators are waiting for the door to be introduced \n",
            "Epoch 19 Loss 0.8730 Accuracy 0.2911\n",
            "Time taken for 1 epoch: 187.49423933029175 secs\n",
            "\n",
            "Input: son honneur le president informe le senat que des senateurs attendent a la porte pour etre presentes\n",
            "Predicted answer: the hon the speaker informed the senate that senators shall be waiting for the door to present \n",
            "Epoch 20 Loss 0.8674 Accuracy 0.2918\n",
            "Time taken for 1 epoch: 187.56879830360413 secs\n",
            "\n",
            "Input: son honneur le president informe le senat que des senateurs attendent a la porte pour etre presentes\n",
            "Predicted answer: the hon the speaker informed the senate that senators are waiting to be introduced \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi_j9DMFX3Ez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer.save_weights(\"transformer\")"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3YPFHJ0qeFE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer2 = Transformer(num_layers=num_layers, num_heads=num_heads, d_model=d_model, dense_dim=dense_dim,\n",
        "                          in_vocab_size=fr_vocab_size, tar_vocab_size=en_vocab_size,\n",
        "                          input_max_position=max_len_fr, target_max_position=max_len_en, rate=0.1)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVnIJ-XUrMZi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a0be0da2-a378-46b7-c887-573560271e21"
      },
      "source": [
        "transformer.load_weights(\"transformer\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f02e0e67438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shtEORFRsTGY",
        "colab_type": "text"
      },
      "source": [
        "We compare the translation with the data (preprocessed)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ir2LZ8TrQoF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "0da4dacd-b758-47dc-89bf-90928d9bef81"
      },
      "source": [
        "translate(\"son excellence le gouverneur general etant arrive au senat et ayant pris place sur le trone\")\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: son excellence le gouverneur general etant arrive au senat et ayant pris place sur le trone\n",
            "Predicted answer: his excellency the governor general came to the senate and having placed on the throne  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O7ZkFaGrb3l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "659cb7ac-79ca-449a-9276-47c59bebab84"
      },
      "source": [
        "print(\"French: Son Excellence le Gouverneur général étant arrivé au Sénat et ayant pris place sur le trône\")\n",
        "print(\"English: His Excellency the Governor General having come and being seated upon the Throne\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "French: Son Excellence le Gouverneur général étant arrivé au Sénat et ayant pris place sur le trône\n",
            "English: His Excellency the Governor General having come and being seated upon the Throne\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}