{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF_Transformer_FR_EN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyle-gao/TF_Transformer/blob/master/TF_Transformer_FR_EN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anQuvdlJJ8_r"
      },
      "source": [
        "Copyright 2020 Yi Lin(Kyle) Gao\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_qNSzzyaCbD"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "jmjh290raIky"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFKnTnn7Odcp"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import time\n",
        "import shutil"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8T0wbs4fNbbU",
        "outputId": "9edc8c24-dc0f-4c09-d1b4-f826b7e7620c"
      },
      "source": [
        "!gdown --id 1NVO3F5mTPhnM34BR3ANvlp-ZeQm3i_-G\n",
        "shutil.unpack_archive(\"/content/Data.zip\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1NVO3F5mTPhnM34BR3ANvlp-ZeQm3i_-G\n",
            "To: /content/Data.zip\n",
            "\r0.00B [00:00, ?B/s]\r4.72MB [00:00, 22.0MB/s]\r11.1MB [00:00, 41.9MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksjz9m2nXDaz"
      },
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dense_dim = 256\n",
        "num_heads = 8\n",
        "\n",
        "\n",
        "max_len_en = 50\n",
        "min_len_en = 10  # The transcript has many short lines indicating the date or speaker, which we should filter out.\n",
        "max_len_fr = 50\n",
        "min_len_fr = 10\n",
        "\n",
        "eng_path = \"Data/en.txt\"\n",
        "fr_path = \"Data/fr.txt\"\n",
        "EPOCHS = 20\n",
        "en_ds = tf.data.TextLineDataset(eng_path)\n",
        "fr_ds = tf.data.TextLineDataset(fr_path)\n",
        "ds = tf.data.Dataset.zip((fr_ds, en_ds))\n",
        "\n",
        "#tokenizer_en = tfds.features.text.SubwordTextEncoder.load_from_file(\"Data/tokenizer_en\")\n",
        "#tokenizer_fr = tfds.features.text.SubwordTextEncoder.load_from_file(\"Data/tokenizer_fr\")\n",
        "\n",
        "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.load_from_file(\"Data/tokenizer_en\")\n",
        "tokenizer_fr = tfds.deprecated.text.SubwordTextEncoder.load_from_file(\"Data/tokenizer_fr\")\n",
        "\n",
        "en_vocab_size = tokenizer_en.vocab_size + 2\n",
        "fr_vocab_size = tokenizer_fr.vocab_size + 2"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFvRld90XAKc"
      },
      "source": [
        "def encode(fr, eng):\n",
        "    # Adds start token (tokenizer.vocab_size) and end token (tokenizer.vocab_size + 1) to (question,answer)\n",
        "    question = [tokenizer_fr.vocab_size] + tokenizer_fr.encode(fr.numpy()) + [tokenizer_fr.vocab_size + 1]\n",
        "    answer = [tokenizer_en.vocab_size] + tokenizer_en.encode(eng.numpy()) + [tokenizer_en.vocab_size + 1]\n",
        "\n",
        "    return question, answer\n",
        "\n",
        "\n",
        "def tf_interleave_encode(question, answer):\n",
        "    # We have to wrap encode in a tf.py_function() and return a Dataset so we can use Dataset.interleave()\n",
        "    question, answer = tf.py_function(encode, [question, answer], [tf.int64, tf.int64])\n",
        "    question.set_shape([None])\n",
        "    answer.set_shape([None])\n",
        "\n",
        "    return tf.data.Dataset.from_tensors((question, answer))\n",
        "\n",
        "\n",
        "def filter_max_length(x, y, max_length_question=max_len_fr, max_length_answer=max_len_en):\n",
        "    return tf.logical_and(tf.size(x) <= max_length_question,\n",
        "                          tf.size(y) <= max_length_answer)\n",
        "\n",
        "\n",
        "def filter_min_length(x, y, min_len_question=min_len_fr, min_len_answer=min_len_en):\n",
        "    return tf.logical_and(tf.size(x) >= min_len_question,\n",
        "                          tf.size(y) >= min_len_answer)\n",
        "\n",
        "\n",
        "def preprocess(dataset, batch_size, pad_len_question=max_len_fr, pad_length_answer=max_len_en):\n",
        "    dataset = dataset.cache()\n",
        "    # dataset = dataset.map(tf_encode)\n",
        "    dataset = dataset.interleave(tf_interleave_encode, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    dataset = dataset.filter(filter_max_length)\n",
        "    dataset = dataset.filter(filter_min_length)\n",
        "    dataset = dataset.shuffle(10000)\n",
        "\n",
        "    dataset = dataset.padded_batch(batch_size, drop_remainder=True,\n",
        "                                   padded_shapes=([pad_len_question], [pad_length_answer]))\n",
        "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return dataset"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXcg2N4yYKur"
      },
      "source": [
        "train_dataset = preprocess(ds, 64)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_1FxrOUYEo9"
      },
      "source": [
        "def positional_encoding(pos, d_model):\n",
        "    \"\"\"\n",
        "    :param pos: int max position\n",
        "    :param d_model: dimension of the model\n",
        "    :return: (1,pos,d_model) array of sinusoidal positional encoding\n",
        "    \"\"\"\n",
        "    pos_enc = np.zeros((1, pos, d_model))\n",
        "    for p in range(pos):\n",
        "        for i in range(d_model // 2):\n",
        "            angles = p / np.power(10000, (2 * i) / np.float32(d_model))\n",
        "            pos_enc[:, p, 2 * i] = np.sin(angles)\n",
        "            pos_enc[:, p, 2 * i + 1] = np.cos(angles)\n",
        "        if d_model % 2 == 1:\n",
        "            # if d_model is odd loop doesn't hit last even index\n",
        "            angles = p / np.power(10000, (2 * d_model) / np.float32(d_model))\n",
        "            pos_enc[:, p, d_model - 1] = np.sin(angles)\n",
        "    return tf.cast(pos_enc, tf.float32)\n",
        "\n",
        "\n",
        "def padding_mask(seq):\n",
        "    # Returns (batch, seq_len, 1, 1) tensor with 1's where the sequence is padded, 0 where it is not\n",
        "\n",
        "    mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return mask[:, tf.newaxis, :,  tf.newaxis]  # (batch, 1, seq_len, 1) m l j h  <- j gets masked\n",
        "\n",
        "\n",
        "def forward_mask(seq):\n",
        "    \"\"\"\n",
        "    Calculates a combined forward mask and padding mask for a batch of sequences\n",
        "    :param seq: (batch,seq_len) a batch of sequences\n",
        "    :return:  a combined look_ahead_mask (upper triangular 1s)\n",
        "    and padding mask (batch, seq_len, seq_len, 1)\n",
        "    \"\"\"\n",
        "    seq_len = tf.shape(seq)[1]\n",
        "\n",
        "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "    look_ahead_mask = look_ahead_mask[tf.newaxis, :, :, tf.newaxis]  # (batch, seq_len, seq_len, 1)\n",
        "\n",
        "    padded_mask = padding_mask(seq)\n",
        "\n",
        "    # return padded_mask * look_ahead_mask  # (batch, seq_len, seq_len, 1)\n",
        "    return tf.maximum(padded_mask, look_ahead_mask)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwFRZhdwWwK_"
      },
      "source": [
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "    \"\"\"Implemented with tf.einsum(), is faster than using tf.transpose() with tf.matmul()\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads,depth)\n",
        "\n",
        "        Arguments:\n",
        "        x -- A tokenized sequence (batch_size, seq_len, d_model)\n",
        "\n",
        "        Returns:\n",
        "        A tokenized sequence with dimensions (batch_size, seq_len, num_heads, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "\n",
        "        return x\n",
        "\n",
        "    def call(self, q, k, v, mask=None):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "        q = self.wq(q)  # (batch_size,len_q, dim_q)\n",
        "        k = self.wk(k)  # (batch_size,len_v, dim_q)\n",
        "        v = self.wv(v)  # (batch_size,len_v, dim_v)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, len_q, num_heads, depth_q) (m,l,h,d)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, len_v, num_heads, depth_q) (m,j,h,d)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, len_v, num_heads, depth_v) (m,j,h,e)\n",
        "\n",
        "        qk = tf.einsum(\"mlhd,mjhd->mljh\", q, k)  # (batch_size, len_q, len_v, num_heads) (m,l,j,h)\n",
        "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "        qk = qk / tf.math.sqrt(dk)\n",
        "\n",
        "        if mask is not None:\n",
        "            qk = qk - mask*1e9 # We are using a additive mask\n",
        "\n",
        "        qk = tf.nn.softmax(qk, axis=-2)  # (batch_size,len_q,len_v, num_heads) (m,l,j,h)\n",
        "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "        qk = qk / tf.math.sqrt(dk)\n",
        "\n",
        "        output = tf.einsum(\"mljh, mjhe -> mlhe\", qk, v)  # (batch_size,len_q, heads, depth_v)\n",
        "        output = tf.reshape(output, (batch_size, -1, self.num_heads * self.depth))  # (batch_size,len_q, d_model)\n",
        "\n",
        "        return self.dense(output)\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"The EncoderLayer consists of one MultiHeadAttention layer connected to a FeedForward layer,\n",
        "    each of these 2 layers have a residual connection.\"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, d_model, dense_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.dense = tf.keras.Sequential([tf.keras.layers.Dense(dense_dim, activation='relu'),\n",
        "                                          tf.keras.layers.Dense(d_model)])\n",
        "\n",
        "        self.norm1 = tf.keras.layers.LayerNormalization()\n",
        "        self.norm2 = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(dropout)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        out_attention = self.attention(x, x, x, mask)  # (batch_size,seq_len,d_model)\n",
        "        out_attention = self.dropout1(out_attention, training=training)\n",
        "        out1 = self.norm1(x + out_attention)  # residual connection (batch_size,seq_len,d_model)\n",
        "\n",
        "        out_dense = self.dense(out1)  # (batch_size,seq_len,d_model)\n",
        "        out2 = self.norm2(out1 + out_dense)  # residual conenction (batch_size,seq_len,d_model)\n",
        "        return out2\n",
        "\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    \"\"\"The Encoder consists of EncoderLayer\"\"\"\n",
        "\n",
        "    def __init__(self, num_layers, num_heads, d_model, dense_dim,\n",
        "                 vocab_size, max_encoding_position, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = positional_encoding(max_encoding_position, d_model)\n",
        "        self.encoding_layers = [EncoderLayer(num_heads, d_model, dense_dim, dropout) for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, x, training, mask=None):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        x = self.embedding(x)  # (batch_size,input_len,d_model)\n",
        "        x = x * tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x = x + self.positional_encoding[:, :seq_len, :]\n",
        "        x = self.dropout(x, training=training)\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.encoding_layers[i](x, training, mask)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        return x\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"A decoder layers consists of two MultiHeadAttention, one for the Decoder input, one from Encoder output\"\"\"\n",
        "    def __init__(self, num_heads, d_model, dense_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attention1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.attention2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.dense = tf.keras.Sequential([tf.keras.layers.Dense(dense_dim, activation='relu'),\n",
        "                                          tf.keras.layers.Dense(d_model)])\n",
        "\n",
        "        self.norm1 = tf.keras.layers.LayerNormalization()\n",
        "        self.norm2 = tf.keras.layers.LayerNormalization()\n",
        "        self.norm3 = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(dropout)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(dropout)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, encoder_out, x, training, forward_mask, padding_mask):\n",
        "\n",
        "        out_attention1 = self.attention1(x, x, x,\n",
        "                                         forward_mask)  # (batch_size, seq_len_answer, d_model) -> The return seq_len is the same as that of the first argument of the call.\n",
        "        out_attention1 = self.dropout1(out_attention1, training=training)\n",
        "        out1 = self.norm1(x + out_attention1)  # residual connection (batch_size, seq_len_answer, d_model)\n",
        "\n",
        "        out_attention2 = self.attention2(out1, encoder_out, encoder_out,\n",
        "                                         padding_mask)  # (batch_size, seq_len_answer, d_model)\n",
        "        out_attention2 = self.dropout2(out_attention2, training=training)\n",
        "        out2 = self.norm2(out1 + out_attention2)\n",
        "\n",
        "        out_dense = self.dense(out2)\n",
        "        out_dense = self.dropout3(out_dense + out2)\n",
        "\n",
        "        return out_dense\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    \"\"\"The Decoder consists of multiple DecoderLayer\"\"\"\n",
        "    def __init__(self, num_layers, num_heads, d_model, dense_dim,\n",
        "                 vocab_size, max_encoding_position, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = positional_encoding(max_encoding_position, d_model)\n",
        "        self.decoder_layers = [DecoderLayer(num_heads, d_model, dense_dim, dropout) for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, encoder_out, x, training, forward_mask=None, padding_mask=None):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        x = self.embedding(x)  # (batch_size,input_len,d_model)\n",
        "        x = x * tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x = x + self.positional_encoding[:, :seq_len, :]\n",
        "        x = self.dropout(x, training=training)\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.decoder_layers[i](encoder_out, x, training, forward_mask,\n",
        "                                       padding_mask)  # (batch_size, input_seq_len, d_model)\n",
        "        return x\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, num_layers, num_heads, d_model, dense_dim, in_vocab_size, tar_vocab_size,\n",
        "                 input_max_position, target_max_position, rate=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, num_heads, d_model, dense_dim,\n",
        "                               in_vocab_size, max_encoding_position=input_max_position, dropout=0.1)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, num_heads, d_model, dense_dim,\n",
        "                               tar_vocab_size, max_encoding_position=target_max_position, dropout=0.1)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(tar_vocab_size)\n",
        "\n",
        "    def call(self, input, target, training=False, enc_mask=None, dec_forward_mask=None, dec_padding_mask=None):\n",
        "        out_encoder = self.encoder(input, training=training, mask=enc_mask)\n",
        "\n",
        "        out_decoder = self.decoder(out_encoder, target, training=training, forward_mask=dec_forward_mask,\n",
        "                                   padding_mask=dec_padding_mask)\n",
        "\n",
        "        out = self.dense(out_decoder)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTSPidClYyzs"
      },
      "source": [
        "transformer = Transformer(num_layers=num_layers, num_heads=num_heads, d_model=d_model, dense_dim=dense_dim,\n",
        "                          in_vocab_size=fr_vocab_size, tar_vocab_size=en_vocab_size,\n",
        "                          input_max_position=max_len_fr, target_max_position=max_len_en, rate=0.1)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQlMu9Z_WzpI"
      },
      "source": [
        "def evaluate(question):\n",
        "\n",
        "    start_token = [tokenizer_fr.vocab_size]\n",
        "    end_token = [tokenizer_fr.vocab_size + 1]\n",
        "    question = start_token + tokenizer_fr.encode(question) + end_token\n",
        "    question = tf.expand_dims(question, 0)\n",
        "    answer_in = [tokenizer_en.vocab_size]\n",
        "    answer_in = tf.expand_dims(answer_in, 0)\n",
        "\n",
        "    for i in range(max_len_fr):\n",
        "        enc_padding_mask = padding_mask(question)\n",
        "        dec_padding_mask = padding_mask(question)\n",
        "        dec_forward_mask = forward_mask(answer_in)\n",
        "\n",
        "        predictions = transformer(question, answer_in, training=False, enc_mask=enc_padding_mask,\n",
        "                                  dec_forward_mask=dec_forward_mask, dec_padding_mask=dec_padding_mask)\n",
        "        prediction = predictions[:, -1, :]  # select the last word to add to the outputs\n",
        "\n",
        "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
        "\n",
        "        if predicted_id == tokenizer_en.vocab_size + 1:\n",
        "            return tf.squeeze(answer_in, axis=0)\n",
        "        predicted_id = tf.expand_dims(predicted_id, 0)\n",
        "        answer_in = tf.concat([answer_in, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(answer_in, axis=0)\n",
        "\n",
        "\n",
        "def translate(sentence):\n",
        "    result = np.array(evaluate(sentence))\n",
        "\n",
        "    predicted_sentence = tokenizer_en.decode([i for i in result\n",
        "                                              if tokenizer_en.vocab_size > i > 0])\n",
        "    print('Input: {}'.format(sentence))\n",
        "    print('Predicted answer: {}'.format(predicted_sentence))\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABu70dFYXSUt"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "        def __init__(self, d_model, warmup_steps=4000):\n",
        "            super(CustomSchedule, self).__init__()\n",
        "\n",
        "            self.d_model = d_model\n",
        "            self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "            self.warmup_steps = warmup_steps\n",
        "\n",
        "        def __call__(self, step):\n",
        "            arg1 = tf.math.rsqrt(step)\n",
        "            arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "            return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "learning_rate = CustomSchedule(d_model, warmup_steps=4000)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
        "                                         epsilon=1e-9)\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "\n",
        "def masked_loss_fn(answer, prediction):\n",
        "        mask = tf.math.logical_not(tf.math.equal(answer, 0))  # 0 at zeroes, 1 at non-zeroes since seq is padded\n",
        "        # mask = tf.math.equal(answer, 0)\n",
        "        mask = tf.cast(mask, tf.int32)\n",
        "        loss_value = loss_fn(answer, prediction,\n",
        "                             sample_weight=mask)  # set the zeros to zero weight, other values have weight of 1.\n",
        "\n",
        "        return loss_value\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "        name='train_accuracy')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OryxpF_Xa11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "outputId": "d71d4a76-1445-47a4-f89c-c37ccdada8ae"
      },
      "source": [
        "    signature = [tf.TensorSpec(shape=(None, max_len_fr), dtype=tf.int64),\n",
        "                 tf.TensorSpec(shape=(None, max_len_en),\n",
        "                               dtype=tf.int64), ]  # a bit faster if we specify the signature\n",
        "\n",
        "    @tf.function(input_signature=signature)\n",
        "    def train_step(question, answer):\n",
        "        answer_in = answer[:, :-1]\n",
        "        answer_tar = answer[:, 1:]\n",
        "\n",
        "        enc_padding_mask = padding_mask(question)\n",
        "        dec_padding_mask = padding_mask(question)\n",
        "        dec_forward_mask = forward_mask(answer_in)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = transformer(question, answer_in, training=True, enc_mask=enc_padding_mask,\n",
        "                                      dec_forward_mask=dec_forward_mask, dec_padding_mask=dec_padding_mask)\n",
        "            loss = masked_loss_fn(answer_tar, predictions)\n",
        "\n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "        train_loss(loss)\n",
        "        train_accuracy(answer_tar, predictions)\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        start = time.time()\n",
        "\n",
        "        train_loss.reset_states()\n",
        "        train_accuracy.reset_states()\n",
        "\n",
        "        for (batch, (question, answer)) in enumerate(train_dataset):\n",
        "            train_step(question, answer)\n",
        "\n",
        "        print('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1,\n",
        "                                                            train_loss.result(),\n",
        "                                                            train_accuracy.result()))\n",
        "\n",
        "        print('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n",
        "        translate(\"son honneur le president informe le senat que des senateurs attendent a la porte pour etre \"\n",
        "                        \"presentes\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss 2.4798 Accuracy 0.1001\n",
            "Time taken for 1 epoch: 567.0949609279633 secs\n",
            "\n",
            "Input: son honneur le president informe le senat que des senateurs attendent a la porte pour etre presentes\n",
            "Predicted answer: his honour the speaker that the senate will be able to be able to be able to be able to be able to be \n",
            "Epoch 2 Loss 1.7021 Accuracy 0.1850\n",
            "Time taken for 1 epoch: 544.6590714454651 secs\n",
            "\n",
            "Input: son honneur le president informe le senat que des senateurs attendent a la porte pour etre presentes\n",
            "Predicted answer: the honour the president of the senate senator who are the senate to be the opportunity to be made \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-ff4d055402e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    745\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m           output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2723\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   2724\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IteratorGetNext\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2725\u001b[0;31m         \"output_shapes\", output_shapes)\n\u001b[0m\u001b[1;32m   2726\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2727\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi_j9DMFX3Ez"
      },
      "source": [
        "transformer.save_weights(\"transformer\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3YPFHJ0qeFE"
      },
      "source": [
        "transformer2 = Transformer(num_layers=num_layers, num_heads=num_heads, d_model=d_model, dense_dim=dense_dim,\n",
        "                          in_vocab_size=fr_vocab_size, tar_vocab_size=en_vocab_size,\n",
        "                          input_max_position=max_len_fr, target_max_position=max_len_en, rate=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVnIJ-XUrMZi"
      },
      "source": [
        "transformer.load_weights(\"transformer\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shtEORFRsTGY"
      },
      "source": [
        "We compare the translation with the data (preprocessed)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ir2LZ8TrQoF"
      },
      "source": [
        "translate(\"son excellence le gouverneur general etant arrive au senat et ayant pris place sur le trone\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O7ZkFaGrb3l"
      },
      "source": [
        "print(\"French: Son Excellence le Gouverneur général étant arrivé au Sénat et ayant pris place sur le trône\")\n",
        "print(\"English: His Excellency the Governor General having come and being seated upon the Throne\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}