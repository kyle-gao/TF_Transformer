{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BLEUScore.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPB+H5EFW/Zmhg/A9U9TB6R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyle-gao/TF_Transformer/blob/master/BLEUScore.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anQuvdlJJ8_r",
        "colab_type": "text"
      },
      "source": [
        "Copyright 2020 Yi Lin(Kyle) Gao\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "jmjh290raIky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdmyX5eVwy7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjNruf5td1HU",
        "colab_type": "text"
      },
      "source": [
        "The validation data consists of the first 25 lines of the 2000-05-09 senate debate (part of the validation set, not used for training) of the Hansards dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg4sXQZ74A23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer_en = tfds.features.text.SubwordTextEncoder.load_from_file(\"Data/tokenizer_en\")\n",
        "tokenizer_fr = tfds.features.text.SubwordTextEncoder.load_from_file(\"Data/tokenizer_fr\")\n",
        "en_vocab_size = tokenizer_en.vocab_size + 2\n",
        "fr_vocab_size = tokenizer_fr.vocab_size + 2"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmFTvJYJ4DZb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def positional_encoding(pos, d_model):\n",
        "    \"\"\"\n",
        "    :param pos: int max position\n",
        "    :param d_model: dimension of the model\n",
        "    :return: (1,pos,d_model) array of sinusoidal positional encoding\n",
        "    \"\"\"\n",
        "    pos_enc = np.zeros((1, pos, d_model))\n",
        "    for p in range(pos):\n",
        "        for i in range(d_model // 2):\n",
        "            angles = p / np.power(10000, (2 * i) / np.float32(d_model))\n",
        "            pos_enc[:, p, 2 * i] = np.sin(angles)\n",
        "            pos_enc[:, p, 2 * i + 1] = np.cos(angles)\n",
        "        if d_model % 2 == 1:\n",
        "            # if d_model is odd loop doesn't hit last even index\n",
        "            angles = p / np.power(10000, (2 * d_model) / np.float32(d_model))\n",
        "            pos_enc[:, p, d_model - 1] = np.sin(angles)\n",
        "    return tf.cast(pos_enc, tf.float32)\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "    \"\"\"Implemented with tf.einsum(), is faster than using tf.transpose() with tf.matmul()\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads,depth)\n",
        "\n",
        "        Arguments:\n",
        "        x -- A tokenized sequence (batch_size, seq_len, d_model)\n",
        "\n",
        "        Returns:\n",
        "        A tokenized sequence with dimensions (batch_size, seq_len, num_heads, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "\n",
        "        return x\n",
        "\n",
        "    def call(self, q, k, v, mask=None):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "        q = self.wq(q)  # (batch_size,len_q, dim_q)\n",
        "        k = self.wk(k)  # (batch_size,len_v, dim_q)\n",
        "        v = self.wv(v)  # (batch_size,len_v, dim_v)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, len_q, num_heads, depth_q) (m,l,h,d)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, len_v, num_heads, depth_q) (m,j,h,d)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, len_v, num_heads, depth_v) (m,j,h,e)\n",
        "\n",
        "        qk = tf.einsum(\"mlhd,mjhd->mljh\", q, k)  # (batch_size, len_q, len_v, num_heads) (m,l,j,h)\n",
        "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "        qk = qk / tf.math.sqrt(dk)\n",
        "\n",
        "        if mask is not None:\n",
        "            qk = qk - mask*1e9 # We are using a multiplicative mask\n",
        "\n",
        "        qk = tf.nn.softmax(qk, axis=-2)  # (batch_size,len_q,len_v, num_heads) (m,l,j,h)\n",
        "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "        qk = qk / tf.math.sqrt(dk)\n",
        "\n",
        "        output = tf.einsum(\"mljh, mjhe -> mlhe\", qk, v)  # (batch_size,len_q, heads, depth_v)\n",
        "        output = tf.reshape(output, (batch_size, -1, self.num_heads * self.depth))  # (batch_size,len_q, d_model)\n",
        "\n",
        "        return self.dense(output)\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"The EncoderLayer consists of one MultiHeadAttention layer connected to a FeedForward layer,\n",
        "    each of these 2 layers have a residual connection.\"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, d_model, dense_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.dense = tf.keras.Sequential([tf.keras.layers.Dense(dense_dim, activation='relu'),\n",
        "                                          tf.keras.layers.Dense(d_model)])\n",
        "\n",
        "        self.norm1 = tf.keras.layers.LayerNormalization()\n",
        "        self.norm2 = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(dropout)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        out_attention = self.attention(x, x, x, mask)  # (batch_size,seq_len,d_model)\n",
        "        out_attention = self.dropout1(out_attention, training=training)\n",
        "        out1 = self.norm1(x + out_attention)  # residual connection (batch_size,seq_len,d_model)\n",
        "\n",
        "        out_dense = self.dense(out1)  # (batch_size,seq_len,d_model)\n",
        "        out2 = self.norm2(out1 + out_dense)  # residual conenction (batch_size,seq_len,d_model)\n",
        "        return out2\n",
        "\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    \"\"\"The Encoder consists of EncoderLayer\"\"\"\n",
        "\n",
        "    def __init__(self, num_layers, num_heads, d_model, dense_dim,\n",
        "                 vocab_size, max_encoding_position, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = positional_encoding(max_encoding_position, d_model)\n",
        "        self.encoding_layers = [EncoderLayer(num_heads, d_model, dense_dim, dropout) for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, x, training, mask=None):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        x = self.embedding(x)  # (batch_size,input_len,d_model)\n",
        "        x = x * tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x = x + self.positional_encoding[:, :seq_len, :]\n",
        "        x = self.dropout(x, training=training)\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.encoding_layers[i](x, training, mask)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        return x\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"A decoder layers consists of two MultiHeadAttention, one for the Decoder input, one from Encoder output\"\"\"\n",
        "    def __init__(self, num_heads, d_model, dense_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attention1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.attention2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.dense = tf.keras.Sequential([tf.keras.layers.Dense(dense_dim, activation='relu'),\n",
        "                                          tf.keras.layers.Dense(d_model)])\n",
        "\n",
        "        self.norm1 = tf.keras.layers.LayerNormalization()\n",
        "        self.norm2 = tf.keras.layers.LayerNormalization()\n",
        "        self.norm3 = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(dropout)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(dropout)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, encoder_out, x, training, forward_mask, padding_mask):\n",
        "\n",
        "        out_attention1 = self.attention1(x, x, x,\n",
        "                                         forward_mask)  # (batch_size, seq_len_answer, d_model) -> The return seq_len is the same as that of the first argument of the call.\n",
        "        out_attention1 = self.dropout1(out_attention1, training=training)\n",
        "        out1 = self.norm1(x + out_attention1)  # residual connection (batch_size, seq_len_answer, d_model)\n",
        "\n",
        "        out_attention2 = self.attention2(out1, encoder_out, encoder_out,\n",
        "                                         padding_mask)  # (batch_size, seq_len_answer, d_model)\n",
        "        out_attention2 = self.dropout2(out_attention2, training=training)\n",
        "        out2 = self.norm2(out1 + out_attention2)\n",
        "\n",
        "        out_dense = self.dense(out2)\n",
        "        out_dense = self.dropout3(out_dense + out2)\n",
        "\n",
        "        return out_dense\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    \"\"\"The Decoder consists of multiple DecoderLayer\"\"\"\n",
        "    def __init__(self, num_layers, num_heads, d_model, dense_dim,\n",
        "                 vocab_size, max_encoding_position, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = positional_encoding(max_encoding_position, d_model)\n",
        "        self.decoder_layers = [DecoderLayer(num_heads, d_model, dense_dim, dropout) for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, encoder_out, x, training, forward_mask=None, padding_mask=None):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        x = self.embedding(x)  # (batch_size,input_len,d_model)\n",
        "        x = x * tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x = x + self.positional_encoding[:, :seq_len, :]\n",
        "        x = self.dropout(x, training=training)\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.decoder_layers[i](encoder_out, x, training, forward_mask,\n",
        "                                       padding_mask)  # (batch_size, input_seq_len, d_model)\n",
        "        return x\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, num_layers, num_heads, d_model, dense_dim, in_vocab_size, tar_vocab_size,\n",
        "                 input_max_position, target_max_position, rate=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, num_heads, d_model, dense_dim,\n",
        "                               in_vocab_size, max_encoding_position=input_max_position, dropout=0.1)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, num_heads, d_model, dense_dim,\n",
        "                               tar_vocab_size, max_encoding_position=target_max_position, dropout=0.1)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(tar_vocab_size)\n",
        "\n",
        "    def call(self, input, target, training=False, enc_mask=None, dec_forward_mask=None, dec_padding_mask=None):\n",
        "        out_encoder = self.encoder(input, training=training, mask=enc_mask)\n",
        "\n",
        "        out_decoder = self.decoder(out_encoder, target, training=training, forward_mask=dec_forward_mask,\n",
        "                                   padding_mask=dec_padding_mask)\n",
        "\n",
        "        out = self.dense(out_decoder)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "def padding_mask(seq):\n",
        "    # Returns (batch, seq_len, 1, 1) tensor with 0's where the sequence is padded, 1 where it is not\n",
        "\n",
        "    mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return mask[:, tf.newaxis, :,  tf.newaxis]  # (batch, 1, seq_len, 1) m l j h  <- j gets masked\n",
        "\n",
        "\n",
        "def forward_mask(seq):\n",
        "    \"\"\"\n",
        "    Calculates a combined forward mask and padding mask for a batch of sequences\n",
        "    :param seq: (batch,seq_len) a batch of sequences\n",
        "    :return:  a combined look_ahead_mask (lower triangular 1s)\n",
        "    and padding mask (batch, seq_len, seq_len, 1)\n",
        "    \"\"\"\n",
        "    seq_len = tf.shape(seq)[1]\n",
        "\n",
        "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "    look_ahead_mask = look_ahead_mask[tf.newaxis, :, :, tf.newaxis]  # (batch, seq_len, seq_len, 1)\n",
        "\n",
        "    padded_mask = padding_mask(seq)\n",
        "\n",
        "    # return padded_mask * look_ahead_mask  # (batch, seq_len, seq_len, 1)\n",
        "    return tf.maximum(padded_mask, look_ahead_mask)\n",
        "\n",
        "    \n",
        "def evaluate(question):\n",
        "\n",
        "    start_token = [tokenizer_fr.vocab_size]\n",
        "    end_token = [tokenizer_fr.vocab_size + 1]\n",
        "    question = start_token + tokenizer_fr.encode(question) + end_token\n",
        "    question = tf.expand_dims(question, 0)\n",
        "    answer_in = [tokenizer_en.vocab_size]\n",
        "    answer_in = tf.expand_dims(answer_in, 0)\n",
        "\n",
        "    for i in range(max_len_fr):\n",
        "        enc_padding_mask = padding_mask(question)\n",
        "        dec_padding_mask = padding_mask(question)\n",
        "        dec_forward_mask = forward_mask(answer_in)\n",
        "\n",
        "        predictions = transformer(question, answer_in, training=False, enc_mask=enc_padding_mask,\n",
        "                                  dec_forward_mask=dec_forward_mask, dec_padding_mask=dec_padding_mask)\n",
        "        prediction = predictions[:, -1, :]  # select the last word to add to the outputs\n",
        "\n",
        "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
        "\n",
        "        if predicted_id == tokenizer_en.vocab_size + 1:\n",
        "            return tf.squeeze(answer_in, axis=0)\n",
        "        predicted_id = tf.expand_dims(predicted_id, 0)\n",
        "        answer_in = tf.concat([answer_in, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(answer_in, axis=0)\n",
        "\n",
        "\n",
        "def translate(sentence):\n",
        "    result = np.array(evaluate(sentence))\n",
        "\n",
        "    predicted_sentence = tokenizer_en.decode([i for i in result\n",
        "                                              if tokenizer_en.vocab_size > i > 0])\n",
        "    print('Input: {}'.format(sentence))\n",
        "    print('Predicted answer: {}'.format(predicted_sentence))\n",
        "    return predicted_sentence"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1XfoZ2XHmaY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ref1_path = \"Data/Val/SenateDebate2000-05-09_e.txt\"\n",
        "ref2_path = \"Data/Val/SenateDebate2000-05-09_GoogleTranslate.txt\"\n",
        "val_path = \"Data/Val/SenateDebate2000-05-09_f.txt\""
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUTZ8AmaSgbT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_string (path):\n",
        "  with open(path) as file:\n",
        "    str_lst = file.readlines()\n",
        "  str_lst = str_lst[:25]\n",
        "  str_lst = [s.rstrip(\"\\n\") for s in str_lst]\n",
        "  return str_lst\n",
        "val_data = process_string(val_path)\n",
        "ref1 = process_string(ref1_path)\n",
        "ref2 = process_string(ref2_path)\n"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yt2eOp8_Tdyt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ref1 = [s.split(\" \") for s in ref1]\n",
        "ref2 = [s.split(\" \") for s in ref2]"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX7JqQTUJzgo",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvxvXysjREhp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b1564d8e-fb3c-485c-da22-690970144cd1"
      },
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dense_dim = 256\n",
        "num_heads = 8\n",
        "\n",
        "\n",
        "max_len_en = 50\n",
        "min_len_en = 10  # The transcript has many short lines indicating the date or speaker, which we should filter out.\n",
        "max_len_fr = 50\n",
        "min_len_fr = 10\n",
        "\n",
        "transformer = Transformer(num_layers=num_layers, num_heads=num_heads, d_model=d_model, dense_dim=dense_dim,\n",
        "                          in_vocab_size=fr_vocab_size, tar_vocab_size=en_vocab_size,\n",
        "                          input_max_position=max_len_fr, target_max_position=max_len_en, rate=0.1)\n",
        "transformer.load_weights(\"transformer\")"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f5c2fb103c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gF9rqjLrXgVW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        },
        "outputId": "e00a588c-48ed-4396-a560-97b413e9a155"
      },
      "source": [
        "translation = [translate(s) for s in val_data]\n",
        "translation=[s.split(\" \") for s in translation]"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: debats du senat (hansard) \n",
            "Predicted answer: senate debates are scandal \n",
            "Input: 2e session, 36e legislature, \n",
            "Predicted answer: 2 the 27th session of parliament is a day of parliament \n",
            "Input: volume 138, numero 53 \n",
            "Predicted answer: the 138883 will be 53 \n",
            "Input: le mardi 9 mai 2000 \n",
            "Predicted answer: on tuesday may 9 2000 \n",
            "Input: l'honorable gildas l. molgat, president \n",
            "Predicted answer: the honourable gildas l molgat president \n",
            "Input: table des matieres \n",
            "Predicted answer: table material \n",
            "Input: declarations de senateurs \n",
            "Predicted answer: statements of senators are made by senators \n",
            "Input: la seconde guerre mondiale \n",
            "Predicted answer: second world war ii \n",
            "Input: le cinquante-cinquieme anniversaire du jour de la victoire en europe \n",
            "Predicted answer: fiftyfifth anniversary of victory in europe \n",
            "Input: la semaine nationale des soins palliatifs \n",
            "Predicted answer: national palliative care week \n",
            "Input: le deces du juge ronald newton pugsley, c.r. \n",
            "Predicted answer: the late justice ron newton ttel the anwe of the cry2ke \n",
            "Input: hommage \n",
            "Predicted answer: tribute to the salute \n",
            "Input: la defense nationale \n",
            "Predicted answer: national defence \n",
            "Input: l'etat de navigabilite des helicopteres sea king-le carnet de vol d'un pilote \n",
            "Predicted answer: state of navy navigatility of sea king helicopter lift cartana carriage \n",
            "Input: la conservation de l'eau douce \n",
            "Predicted answer: conservation of officially known as \n",
            "Input: felicitations a l'occasion de la reception de doctorats honorifiques de l'universite mount allison \n",
            "Predicted answer: congratulations to receptive of the receptive doctor from university cameros \n",
            "Input: le programme d'echange de pages avec la chambre des communes \n",
            "Predicted answer: pages exchange program with house of commons \n",
            "Input: affaires courantes \n",
            "Predicted answer: routine proceedings are routinely \n",
            "Input: la loi sur la responsabilite en matiere maritime \n",
            "Predicted answer: maritime liability act \n",
            "Input: rapport du comite \n",
            "Predicted answer: report of committee \n",
            "Input: premiere lecture \n",
            "Predicted answer: first reading first reading \n",
            "Input: projet de loi visant a changer le nom de la circonscription electorale de rimouski-mitis \n",
            "Predicted answer: a bill to change the name of the electoral district of ankieful mitismitismitis  \n",
            "Input: premiere lecture \n",
            "Predicted answer: first reading first reading \n",
            "Input: l'association interparlementaire canada-france \n",
            "Predicted answer: the movement of interparliamentary association canada is open \n",
            "Input: depot du rapport du groupe canadien de la reunion tenue a paris, france \n",
            "Predicted answer: report of canadian group held in parisfrance tabled \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC43EeHdcOQK",
        "colab_type": "text"
      },
      "source": [
        "Corpus Bleu reference vs google translate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcTd2pNYZZKh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ec719a09-e0c9-4b2f-9f47-a758b5697e84"
      },
      "source": [
        "hypothesis = ref2 #list(corpus) of list(setence) of strings(words)\n",
        "reference = [[s] for s in ref1] #list(corpus) of list of list (list of reference sentences) of strings\n",
        "print(nltk.translate.bleu_score.corpus_bleu(reference, hypothesis)) #corpus bleu for google translation"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.317355540283643\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMeKOqQkdCKq",
        "colab_type": "text"
      },
      "source": [
        "Corpus Bleu reference vs Transformer translation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p_FgbPAaLXP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "54dbc6af-e2ee-493e-9321-77ee2f2dd9a1"
      },
      "source": [
        "hypothesis = translation #list(corpus) of list(setence) of strings(words)\n",
        "reference = [[s] for s in ref1] #list(corpus) of list of list (list of reference sentences) of strings\n",
        "print(nltk.translate.bleu_score.corpus_bleu(reference, hypothesis)) #corpus bleu for google translation"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.21291744785409664\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqyTsnuodbv4",
        "colab_type": "text"
      },
      "source": [
        "Corpus Bleu score using both human and google translation as references"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boihVw8Ec4ya",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0f2180ed-84a2-4556-f2f7-49faa0bc9e10"
      },
      "source": [
        "\n",
        "hypothesis = translation #list(corpus) of list(setence) of strings(words)\n",
        "reference = [[ref1[i], ref2[i]] for i in range(len(ref1))] #list(corpus) of list of list (list of reference sentences) of strings\n",
        "print(nltk.translate.bleu_score.corpus_bleu(reference, hypothesis)) #corpus bleu for google translation"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.301703545283996\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}